from typing import TYPE_CHECKING, Optional, List, Any
import pandas as pd
import numpy as np
import os
from gwaslab.info.g_Log import Log
from gwaslab.qc.qc_decorator import with_logging

if TYPE_CHECKING:
    from gwaslab.g_SumstatsMulti import SumstatsMulti

# Try to import MultiSuSiE, with better error handling
_multisusie_import_error = None
try:
    from gwaslab.extension.multisusie.susiepy_ss import multisusie_rss
except (ImportError, ModuleNotFoundError) as e:
    try:
        from gwaslab.extension.multisusie import multisusie_rss
    except (ImportError, ModuleNotFoundError):
        multisusie_rss = None
        _multisusie_import_error = str(e)
else:
        _multisusie_import_error = None


def _run_multisusie_rss_from_file(
    filepath: str,
    population_sizes: Optional[List[int]] = None,
    varY_list: Optional[List[float]] = None,
    rho: float = 0.75,
    L: int = 10,
    scaled_prior_variance: float = 0.2,
    prior_weights: Optional[np.ndarray] = None,
    standardize: bool = False,
    pop_spec_standardization: bool = True,
    estimate_residual_variance: bool = True,
    estimate_prior_variance: bool = True,
    estimate_prior_method: str = 'early_EM',
    pop_spec_effect_priors: bool = True,
    iter_before_zeroing_effects: int = 5,
    prior_tol: float = 1e-9,
    max_iter: int = 100,
    tol: float = 1e-3,
    verbose: bool = False,
    coverage: float = 0.95,
    min_abs_corr: float = 0,
    float_type: type = np.float32,
    low_memory_mode: bool = False,
    recover_R: bool = False,
    single_population_mac_thresh: int = 20,
    mac_list: Optional[List[np.ndarray]] = None,
    multi_population_maf_thresh: float = 0,
    maf_list: Optional[List[np.ndarray]] = None,
    log: Log = Log(),
    reference_data: Optional[pd.DataFrame] = None,
    group_name: Optional[str] = None,
    study_name: Optional[str] = None,
    **kwargs: Any
) -> pd.DataFrame:
    """
    Run MultiSuSiE RSS from filelist generated by to_mesusie().
    
    This function reads the filelist, loads LD matrices and sumstats for each locus,
    and runs MultiSuSiE on each locus separately, then combines the results.
    
    Parameters
    ----------
    filepath : str
        Path to filelist CSV file generated by to_mesusie()
    population_sizes : Optional[List[int]], optional
        List of sample sizes for each population. If None, will be extracted from
        sumstats files using N column.
    varY_list : Optional[List[float]], optional
        List of outcome variances for each population. If None, defaults to 1.0
        for each population (will be estimated by MultiSuSiE if estimate_residual_variance=True).
    reference_data : Optional[pd.DataFrame], optional
        DataFrame with SNPID, CHR, POS for merging CHR/POS if not in results (default: None)
    group_name : Optional[str], optional
        Group name to add to results (default: None)
    study_name : Optional[str], optional
        Study name to add to results (default: None)
    **kwargs : Any
        Additional parameters passed to _run_multisusie_rss for each locus.
    
    Returns
    -------
    pd.DataFrame
        Combined results from all loci with columns: SNPID, CHR, POS, PIP, 
        CREDIBLE_SET_INDEX, LOCUS, GROUP, STUDY, etc. (processed with CHR/POS merged 
        and GROUP/STUDY set if provided)
    """
    # Log citation
    log.write(" -Citation: Rossen, J., Shi, H., Strober, B. J., Zhang, M. J., Kanai, M., McCaw, Z. R., ... & Price, A. L. (2026). MultiSuSiE improves multi-ancestry fine-mapping in All of Us whole-genome sequencing data. Nature Genetics, 1-10.", verbose=kwargs.get("verbose", True))
    
    log.write(" -Loading filelist from: {}".format(filepath), verbose=kwargs.get("verbose", True))
    filelist = pd.read_csv(filepath, sep="\t")
    
    if len(filelist) == 0:
        log.write(" -Filelist is empty", verbose=kwargs.get("verbose", True))
        return pd.DataFrame()
    
    # Get unique loci
    unique_loci = filelist["LOCUS"].unique() if "LOCUS" in filelist.columns else filelist["SNPID"].unique()
    log.write(" -Found {} unique locus/loci".format(len(unique_loci)), verbose=kwargs.get("verbose", True))
    
    # Get number of studies
    if "STUDY" in filelist.columns:
        nstudy = len(filelist["STUDY"].unique())
    elif "SUBSTUDY" in filelist.columns:
        nstudy = filelist["SUBSTUDY"].max()
    else:
        raise ValueError("Cannot determine number of studies from filelist")
    
    log.write(" -Number of studies: {}".format(nstudy), verbose=kwargs.get("verbose", True))
    
    all_results = []
    
    # Process each locus
    for locus_idx, locus in enumerate(unique_loci):
        log.write(" -Processing locus {} ({}/{})...".format(locus, locus_idx + 1, len(unique_loci)), 
                 verbose=kwargs.get("verbose", True))
        
        # Get rows for this locus
        if "LOCUS" in filelist.columns:
            locus_rows = filelist[filelist["LOCUS"] == locus]
        else:
            locus_rows = filelist[filelist["SNPID"] == locus]
        
        if len(locus_rows) != nstudy:
            log.warning("  -Locus {} has {} rows, expected {}. Skipping...".format(
                locus, len(locus_rows), nstudy
            ), verbose=kwargs.get("verbose", True))
            continue
        
        # Sort by SUBSTUDY or STUDY to ensure consistent order
        if "SUBSTUDY" in locus_rows.columns:
            locus_rows = locus_rows.sort_values("SUBSTUDY")
        elif "STUDY" in locus_rows.columns:
            locus_rows = locus_rows.sort_values("STUDY")
        
        # Load LD matrices and sumstats
        R_list = []
        b_list = []
        s_list = []
        variant_ids_list = []
        locus_population_sizes = []
        
        for idx, row in locus_rows.iterrows():
            # Load LD matrix (CSV format, tab-separated, no header)
            ld_path = row["LD_R_MATRIX"]
            if not os.path.exists(ld_path):
                log.warning("  -LD matrix file not found: {}. Skipping locus...".format(ld_path),
                           verbose=kwargs.get("verbose", True))
                break
            
            log.write("  -Loading LD matrix from: {}".format(ld_path), verbose=kwargs.get("verbose", True))
            R = pd.read_csv(ld_path, sep="\t", header=None).values.astype(float_type)
            R_list.append(R)
            
            # Load sumstats (may be compressed)
            sumstats_path = row["LOCUS_SUMSTATS"]
            if not os.path.exists(sumstats_path):
                log.warning("  -Sumstats file not found: {}. Skipping locus...".format(sumstats_path),
                           verbose=kwargs.get("verbose", True))
                break
            
            log.write("  -Loading sumstats from: {}".format(sumstats_path), verbose=kwargs.get("verbose", True))
            # Handle compressed files
            if sumstats_path.endswith('.gz'):
                import gzip
                sumstats = pd.read_csv(sumstats_path, sep="\t", compression='gzip')
            else:
                sumstats = pd.read_csv(sumstats_path, sep="\t")
            
            # Extract variant IDs (tofinemapping_m exports SNPID as "SNP")
            if "SNP" in sumstats.columns:
                variant_ids = sumstats["SNP"].values.tolist()
            elif "SNPID" in sumstats.columns:
                variant_ids = sumstats["SNPID"].values.tolist()
            else:
                # Log available columns for debugging
                log.warning("  -Available columns in sumstats: {}".format(list(sumstats.columns)), 
                           verbose=kwargs.get("verbose", True))
                raise ValueError("SNP or SNPID column not found in sumstats file: {}. Available columns: {}".format(
                    sumstats_path, list(sumstats.columns)
                ))
            
            # Extract BETA and SE
            if "Beta" in sumstats.columns:
                beta_col = "Beta"
            elif "BETA" in sumstats.columns:
                beta_col = "BETA"
            else:
                raise ValueError("BETA column not found in sumstats file: {}".format(sumstats_path))
            
            if "Se" in sumstats.columns:
                se_col = "Se"
            elif "SE" in sumstats.columns:
                se_col = "SE"
            else:
                raise ValueError("SE column not found in sumstats file: {}".format(sumstats_path))
            
            beta = sumstats[beta_col].values.astype(float_type)
            se = sumstats[se_col].values.astype(float_type)
            
            # Handle NaN values
            beta = np.where(np.isnan(beta), 0.0, beta)
            se = np.where(np.isnan(se), 0.0, se)
            se = np.where(se == 0.0, 1e-6, se)  # Replace zero SE with small value
            
            b_list.append(beta)
            s_list.append(se)
            variant_ids_list.append(variant_ids)
            
            # Extract sample size
            if population_sizes is None:
                if "N" in sumstats.columns:
                    n_val = sumstats["N"].mean()
                    if pd.isna(n_val):
                        n_val = 10000  # Default fallback
                    locus_population_sizes.append(int(n_val))
                else:
                    locus_population_sizes.append(10000)  # Default fallback
            else:
                # Use provided population sizes (assuming same order as studies)
                study_idx = row.get("SUBSTUDY", idx) - 1 if "SUBSTUDY" in row else idx
                if study_idx < len(population_sizes):
                    locus_population_sizes.append(population_sizes[study_idx])
                else:
                    locus_population_sizes.append(10000)  # Default fallback
        
        # Check if we successfully loaded all data
        if len(R_list) != nstudy or len(b_list) != nstudy:
            log.warning("  -Failed to load all data for locus {}. Skipping...".format(locus),
                       verbose=kwargs.get("verbose", True))
            continue
        
        # Ensure all variant lists are the same (they should be for matched variants)
        if len(set(len(vids) for vids in variant_ids_list)) > 1:
            log.warning("  -Variant lists have different lengths for locus {}. Using first study's variants...".format(locus),
                       verbose=kwargs.get("verbose", True))
        
        variant_ids = variant_ids_list[0]
        
        # Validate dimensions
        n_variants = len(variant_ids)
        for i, R in enumerate(R_list):
            if R.shape[0] != n_variants or R.shape[1] != n_variants:
                log.warning("  -LD matrix {} has shape {} but expected ({}, {}) for locus {}. Skipping...".format(
                    i+1, R.shape, n_variants, n_variants, locus
                ), verbose=kwargs.get("verbose", True))
                break
        else:
            # All checks passed, run MultiSuSiE
            log.write("  -Running MultiSuSiE for locus {} ({} variants)...".format(locus, n_variants),
                     verbose=kwargs.get("verbose", True))
            
            # Generate varY_list (required when b_list and s_list are provided)
            if varY_list is None:
                # Default to 1.0 for each population (will be estimated by MultiSuSiE if estimate_residual_variance=True)
                locus_varY_list = [1.0] * nstudy
                log.write("  -varY_list not provided, using default value of 1.0 for each population", 
                         verbose=kwargs.get("verbose", True))
            else:
                # Use provided varY_list
                if len(varY_list) != nstudy:
                    log.warning("  -varY_list length ({}) does not match number of studies ({}). Using first {} values...".format(
                        len(varY_list), nstudy, nstudy
                    ), verbose=kwargs.get("verbose", True))
                    locus_varY_list = varY_list[:nstudy] if len(varY_list) >= nstudy else varY_list + [1.0] * (nstudy - len(varY_list))
                else:
                    locus_varY_list = varY_list
                log.write("  -varY_list provided: {}".format([f'{v:.6f}' for v in locus_varY_list]), 
                         verbose=kwargs.get("verbose", True))
            
            # Convert scalar rho to correlation matrix if needed
            if np.isscalar(rho):
                K = nstudy
                rho_matrix = np.full((K, K), float(rho), dtype=float_type)
                np.fill_diagonal(rho_matrix, 1.0)
                rho_locus = rho_matrix
            else:
                rho_locus = np.asarray(rho, dtype=float_type)
            
            # Run MultiSuSiE
            old_settings = np.seterr(all='ignore')
            try:
                result = multisusie_rss(
                    R_list=R_list,
                    population_sizes=locus_population_sizes,
                    b_list=b_list,
                    s_list=s_list,
                    z_list=None,
                    varY_list=locus_varY_list,
                    rho=rho_locus,
                    L=L,
                    scaled_prior_variance=scaled_prior_variance,
                    prior_weights=prior_weights,
                    standardize=standardize,
                    pop_spec_standardization=pop_spec_standardization,
                    estimate_residual_variance=estimate_residual_variance,
                    estimate_prior_variance=estimate_prior_variance,
                    estimate_prior_method=estimate_prior_method,
                    pop_spec_effect_priors=pop_spec_effect_priors,
                    iter_before_zeroing_effects=iter_before_zeroing_effects,
                    prior_tol=prior_tol,
                    max_iter=max_iter,
                    tol=tol,
                    verbose=verbose,
                    coverage=coverage,
                    min_abs_corr=min_abs_corr,
                    float_type=float_type,
                    low_memory_mode=low_memory_mode,
                    recover_R=recover_R,
                    single_population_mac_thresh=single_population_mac_thresh,
                    mac_list=mac_list,
                    multi_population_maf_thresh=multi_population_maf_thresh,
                    maf_list=maf_list,
                    variant_ids=variant_ids,
                    **kwargs
                )
            finally:
                np.seterr(**old_settings)
            
            # Convert results to DataFrame
            results_df = pd.DataFrame({
                "SNPID": variant_ids,
                "PIP": result.pip
            })
            
            # Add CHR and POS from sumstats if available
            if len(locus_rows) > 0:
                first_sumstats_path = locus_rows.iloc[0]["LOCUS_SUMSTATS"]
                if first_sumstats_path.endswith('.gz'):
                    import gzip
                    first_sumstats = pd.read_csv(first_sumstats_path, sep="\t", compression='gzip')
                else:
                    first_sumstats = pd.read_csv(first_sumstats_path, sep="\t")
                
                # Use "SNP" (exported name) or "SNPID" as index
                snp_col = "SNP" if "SNP" in first_sumstats.columns else "SNPID"
                
                if "CHR" in first_sumstats.columns:
                    chr_map = first_sumstats.set_index(snp_col)["CHR"].to_dict()
                    results_df["CHR"] = results_df["SNPID"].map(chr_map)
                if "POS" in first_sumstats.columns:
                    pos_map = first_sumstats.set_index(snp_col)["POS"].to_dict()
                    results_df["POS"] = results_df["SNPID"].map(pos_map)
            
            # Add credible set information
            if hasattr(result, 'sets') and result.sets is not None:
                results_df["CREDIBLE_SET_INDEX"] = 0
                results_df["CS_CATEGORY"] = None
                
                if len(result.sets) >= 1 and len(result.sets[0]) > 0:
                    for cs_idx, variant_indices in enumerate(result.sets[0]):
                        if variant_indices is not None and len(variant_indices) > 0:
                            results_df.loc[variant_indices, "CREDIBLE_SET_INDEX"] = cs_idx + 1
            
            # Add locus metadata
            results_df["LOCUS"] = locus
            # Set GROUP and STUDY from filelist if available (will be overridden later if group_name/study_name provided)
            if "GROUP" in locus_rows.columns:
                results_df["GROUP"] = locus_rows.iloc[0]["GROUP"]
            if "STUDY" in locus_rows.columns:
                studies = locus_rows["STUDY"].unique().tolist()
                results_df["STUDY"] = ", ".join(studies)
            
            all_results.append(results_df)
            log.write("  -Completed locus {}: {} variants, {} credible sets".format(
                locus, len(results_df), len(results_df[results_df["CREDIBLE_SET_INDEX"] > 0].groupby("CREDIBLE_SET_INDEX"))
            ), verbose=kwargs.get("verbose", True))
    
    # Combine all results
    if len(all_results) == 0:
        log.write(" -No results generated", verbose=kwargs.get("verbose", True))
        return pd.DataFrame()
    
    combined_results = pd.concat(all_results, ignore_index=True)
    log.write(" -Combined results from {} loci: {} total variants".format(
        len(all_results), len(combined_results)
    ), verbose=kwargs.get("verbose", True))
    
    # Post-process results: merge CHR/POS, set GROUP/STUDY
    if not combined_results.empty:
        # Merge CHR and POS from reference data if not present
        if reference_data is not None:
            if "CHR" not in combined_results.columns or "POS" not in combined_results.columns:
                log.write(" -Merging CHR and POS from reference data...", 
                         verbose=kwargs.get("verbose", True))
                combined_results = pd.merge(
                    combined_results,
                    reference_data[["SNPID", "CHR", "POS"]],
                    on="SNPID",
                    how="left"
                )
        
        # Set GROUP and STUDY columns if provided (overrides filelist values if provided)
        if group_name is not None:
            combined_results["GROUP"] = group_name
        if study_name is not None:
            combined_results["STUDY"] = study_name
    
    return combined_results


@with_logging(
    start_to_msg="run finemapping using MultiSuSiE RSS",
    finished_msg="running finemapping using MultiSuSiE RSS",
    start_cols=["SNPID", "CHR", "POS"],
    start_function=".run_multisusie_rss()",
    check_tools=["python"]
)
def _run_multisusie_rss(
    gls: Optional['SumstatsMulti'] = None,
    R_list: Optional[List[np.ndarray]] = None,
    filepath: Optional[str] = None,
    population_sizes: Optional[List[int]] = None,
    b_list: Optional[List[np.ndarray]] = None,
    s_list: Optional[List[np.ndarray]] = None,
    z_list: Optional[List[np.ndarray]] = None,
    varY_list: Optional[List[float]] = None,
    rho: float = 0.75,
    L: int = 10,
    scaled_prior_variance: float = 0.2,
    prior_weights: Optional[np.ndarray] = None,
    standardize: bool = False,
    pop_spec_standardization: bool = True,
    estimate_residual_variance: bool = True,
    estimate_prior_variance: bool = True,
    estimate_prior_method: str = 'early_EM',
    pop_spec_effect_priors: bool = True,
    iter_before_zeroing_effects: int = 5,
    prior_tol: float = 1e-9,
    max_iter: int = 100,
    tol: float = 1e-3,
    verbose: bool = False,
    coverage: float = 0.95,
    min_abs_corr: float = 0,
    float_type: type = np.float32,
    low_memory_mode: bool = False,
    recover_R: bool = False,
    single_population_mac_thresh: int = 20,
    mac_list: Optional[List[np.ndarray]] = None,
    multi_population_maf_thresh: float = 0,
    maf_list: Optional[List[np.ndarray]] = None,
    variant_ids: Optional[List[str]] = None,
    log: Log = Log(),
    **kwargs: Any
) -> pd.DataFrame:
    """
    Run finemapping using MultiSuSiE RSS (summary statistics version).
    
    This function runs MultiSuSiE on multiple populations/studies using summary statistics.
    It can work in two modes:
    1. Direct mode: Provide gls and R_list directly
    2. File-based mode: Provide filepath from to_mesusie() to load LD matrices and sumstats
    
    Parameters
    ----------
    gls : Optional[SumstatsMulti]
        SumstatsMulti object containing multi-study summary statistics.
        Required if filepath is not provided.
    R_list : Optional[List[np.ndarray]]
        List of LD correlation matrices, one for each population/study.
        Each matrix should be PxP where P is the number of variants.
        Variants should be in the same order across all matrices.
        Required if filepath is not provided.
    filepath : Optional[str]
        Path to filelist generated by to_mesusie(). If provided, LD matrices and sumstats
        will be loaded from the filelist. Each locus will be processed separately.
        Defaults to None.
    population_sizes : Optional[List[int]], optional
        List of sample sizes for each population. If None, will be extracted from
        the SumstatsMulti object using N_1, N_2, etc. columns.
    b_list : Optional[List[np.ndarray]], optional
        List of effect size arrays (BETA) for each population. If None, will be
        extracted from the SumstatsMulti object.
    s_list : Optional[List[np.ndarray]], optional
        List of standard error arrays (SE) for each population. If None, will be
        extracted from the SumstatsMulti object.
    z_list : Optional[List[np.ndarray]], optional
        List of Z-score arrays for each population. Alternative to b_list/s_list.
    varY_list : Optional[List[float]], optional
        List of outcome variances for each population. If None, will be estimated.
    rho : float, default=0.75
        Effect size correlation parameter between populations.
    L : int, default=10
        Maximum number of causal variants.
    scaled_prior_variance : float, default=0.2
        Scaled prior variance for effect sizes.
    prior_weights : Optional[np.ndarray], optional
        Prior probability of causality for each variant. If None, uniform prior.
    standardize : bool, default=False
        Whether to standardize summary statistics.
    pop_spec_standardization : bool, default=True
        Whether to standardize separately for each population.
    estimate_residual_variance : bool, default=True
        Whether to estimate residual variance.
    estimate_prior_variance : bool, default=True
        Whether to estimate prior variance.
    estimate_prior_method : str, default='early_EM'
        Method for estimating prior variance.
    pop_spec_effect_priors : bool, default=True
        Whether to estimate separate prior variances for each population.
    iter_before_zeroing_effects : int, default=5
        Number of iterations before zeroing out low-likelihood effects.
    prior_tol : float, default=1e-9
        Minimum prior variance tolerance.
    max_iter : int, default=100
        Maximum number of iterations.
    tol : float, default=1e-3
        Convergence tolerance.
    verbose : bool, default=False
        Whether to print progress.
    coverage : float, default=0.95
        Credible set coverage level.
    min_abs_corr : float, default=0
        Minimum absolute correlation for credible sets.
    float_type : type, default=np.float32
        Floating point type to use.
    low_memory_mode : bool, default=False
        Whether to use low memory mode (modifies R_list in place).
    recover_R : bool, default=False
        Whether to recover R matrices from XTX.
    single_population_mac_thresh : int, default=20
        Minor allele count threshold for single population filtering.
    mac_list : Optional[List[np.ndarray]], optional
        List of minor allele counts for each population.
    multi_population_maf_thresh : float, default=0
        Minor allele frequency threshold across all populations.
    maf_list : Optional[List[np.ndarray]], optional
        List of minor allele frequencies for each population.
    variant_ids : Optional[List[str]], optional
        List of variant IDs. If provided, will be included in output.
    log : Log, optional
        Log object for recording progress.
    **kwargs : Any
        Additional keyword arguments passed to multisusie_rss.
    
    Returns
    -------
    pd.DataFrame
        DataFrame containing finemapping results with columns:
        - SNPID: Variant identifier
        - CHR: Chromosome (if available)
        - POS: Position (if available)
        - PIP: Posterior inclusion probability
        - CREDIBLE_SET_INDEX: Credible set index (0 if not in any set)
        - CS_CATEGORY: Credible set category (if available)
        - Additional columns from MultiSuSiE results
    """
    if multisusie_rss is None:
        error_msg = (
            "MultiSuSiE extension not available. Please ensure the multisusie "
            "extension is properly installed in gwaslab.extension.multisusie.\n"
        )
        if _multisusie_import_error:
            error_msg += f"Import error details: {_multisusie_import_error}\n"
            if 'numba' in _multisusie_import_error.lower():
                error_msg += (
                    "It appears that 'numba' is missing. Please install it with: "
                    "pip install numba\n"
                )
            if 'scipy' in _multisusie_import_error.lower():
                error_msg += (
                    "It appears that 'scipy' is missing. Please install it with: "
                    "pip install scipy\n"
                )
        error_msg += (
            "Required dependencies for MultiSuSiE: numba, scipy, tqdm\n"
            "Install with: pip install numba scipy tqdm"
        )
        raise ImportError(error_msg)
    
    # Log citation
    log.write(" -Citation: Rossen, J., Shi, H., Strober, B. J., Zhang, M. J., Kanai, M., McCaw, Z. R., ... & Price, A. L. (2026). MultiSuSiE improves multi-ancestry fine-mapping in All of Us whole-genome sequencing data. Nature Genetics, 1-10.", verbose=kwargs.get("verbose", True))
    
    # Handle filepath mode (from to_mesusie)
    if filepath is not None:
        # Extract reference_data, group_name, study_name from kwargs if provided
        reference_data = kwargs.pop("reference_data", None)
        group_name = kwargs.pop("group_name", None)
        study_name = kwargs.pop("study_name", None)
        
        return _run_multisusie_rss_from_file(
            filepath=filepath,
            population_sizes=population_sizes,
            varY_list=varY_list,
            rho=rho,
            reference_data=reference_data,
            group_name=group_name,
            study_name=study_name,
            L=L,
            scaled_prior_variance=scaled_prior_variance,
            prior_weights=prior_weights,
            standardize=standardize,
            pop_spec_standardization=pop_spec_standardization,
            estimate_residual_variance=estimate_residual_variance,
            estimate_prior_variance=estimate_prior_variance,
            estimate_prior_method=estimate_prior_method,
            pop_spec_effect_priors=pop_spec_effect_priors,
            iter_before_zeroing_effects=iter_before_zeroing_effects,
            prior_tol=prior_tol,
            max_iter=max_iter,
            tol=tol,
            verbose=verbose,
            coverage=coverage,
            min_abs_corr=min_abs_corr,
            float_type=float_type,
            low_memory_mode=low_memory_mode,
            recover_R=recover_R,
            single_population_mac_thresh=single_population_mac_thresh,
            mac_list=mac_list,
            multi_population_maf_thresh=multi_population_maf_thresh,
            maf_list=maf_list,
            log=log,
            **kwargs
        )
    
    # Original direct mode
    if gls is None or R_list is None:
        raise ValueError("Either filepath or both gls and R_list must be provided")
    
    nstudy = gls.meta["gwaslab"]["number_of_studies"]
    log.write(" -Number of studies: {}".format(nstudy), verbose=kwargs.get("verbose", True))
    
    # Extract data
    if gls.engine == "polars":
        data = gls.data.to_pandas()
    else:
        data = gls.data.copy()
    
    # Ensure we have SNPID column
    if "SNPID" not in data.columns:
        if "SNPID_1" in data.columns:
            data["SNPID"] = data["SNPID_1"]
        else:
            raise ValueError("SNPID column not found in data")
    
    # Extract variant IDs if not provided
    if variant_ids is None:
        variant_ids = data["SNPID"].values.tolist()
    
    # Extract BETA, SE, N for each study if not provided
    if b_list is None or s_list is None:
        b_list = []
        s_list = []
        
        log.write(" -Extracting BETA and SE from SumstatsMulti data...", verbose=kwargs.get("verbose", True))
        
        for i in range(nstudy):
            beta_col = "BETA_{}".format(i+1)
            se_col = "SE_{}".format(i+1)
            
            if beta_col not in data.columns or se_col not in data.columns:
                raise ValueError(
                    "BETA_{} or SE_{} columns not found in data. "
                    "Please ensure summary statistics are properly formatted.".format(i+1, i+1)
                )
            
            # Extract as numpy arrays, handling NaN values
            beta_raw = data[beta_col].values.astype(float_type)
            se_raw = data[se_col].values.astype(float_type)
            
            log.write("  -Study {}: Extracted {} variants, {} NaN in BETA, {} NaN in SE".format(
                i+1, len(beta_raw), np.isnan(beta_raw).sum(), np.isnan(se_raw).sum()
            ), verbose=kwargs.get("verbose", True))
            
            # Log causal variants before conversion
            if variant_ids is not None and len(variant_ids) > 38:
                causal_indices = [3, 10, 38]
                log.write("  -Study {}: Causal variants (indices 3, 10, 38) BEFORE NaN conversion:".format(i+1), 
                         verbose=kwargs.get("verbose", True))
                for idx in causal_indices:
                    beta_val = beta_raw[idx] if not np.isnan(beta_raw[idx]) else 'NaN'
                    se_val = se_raw[idx] if not np.isnan(se_raw[idx]) else 'NaN'
                    log.write("    Index {}: BETA={}, SE={}".format(idx, beta_val, se_val), 
                             verbose=kwargs.get("verbose", True))
            
            # Handle NaN and zero SE values
            # Set NaN values to 0
            beta = np.where(np.isnan(beta_raw), 0.0, beta_raw)
            se = np.where(np.isnan(se_raw), 0.0, se_raw)
            
            # Replace zero SE with a small value to avoid division errors
            # Use 1e-6 as a small default SE for variants with zero SE
            se = np.where(se == 0.0, 1e-6, se)
            
            # Log causal variants after conversion
            if variant_ids is not None and len(variant_ids) > 38:
                causal_indices = [3, 10, 38]
                log.write("  -Study {}: Causal variants (indices 3, 10, 38) AFTER NaN conversion:".format(i+1), 
                         verbose=kwargs.get("verbose", True))
                for idx in causal_indices:
                    log.write("    Index {}: BETA={:.6f}, SE={:.6f}".format(idx, beta[idx], se[idx]), 
                             verbose=kwargs.get("verbose", True))
            
            b_list.append(beta)
            s_list.append(se)
    
    # Extract population sizes if not provided
    if population_sizes is None:
        population_sizes = []
        for i in range(nstudy):
            n_col = "N_{}".format(i+1)
            if n_col in data.columns:
                # Use mean N for each study (in case of variable N)
                n_val = data[n_col].mean()
                if pd.isna(n_val):
                    raise ValueError("N_{} column contains only NaN values".format(i+1))
                population_sizes.append(int(n_val))
            else:
                # Try to get from meta
                study_meta = gls.meta["gwaslab"]["objects"].get(i+1, {})
                if "n" in study_meta.get("gwaslab", {}):
                    population_sizes.append(int(study_meta["gwaslab"]["n"]))
                else:
                    raise ValueError(
                        "N_{} column not found and sample size not available in meta".format(i+1)
                    )
    
    log.write(" -Population sizes: {}".format(population_sizes), verbose=kwargs.get("verbose", True))
    
    # Extract varY_list if not provided (estimate from data)
    if varY_list is None:
        log.write("  -varY_list not provided, using default value of 1.0 for each population", 
                 verbose=kwargs.get("verbose", True))
        varY_list = []
        for i in range(nstudy):
            # Estimate varY from BETA and SE if available
            # varY â‰ˆ (BETA^2 + SE^2) for standardized traits
            # For now, set to 1.0 (will be estimated by MultiSuSiE if estimate_residual_variance=True)
            varY_list.append(1.0)
    else:
        log.write("  -varY_list provided: {}".format([f'{v:.6f}' for v in varY_list]), 
                 verbose=kwargs.get("verbose", True))
    
    # Validate R_list
    if len(R_list) != nstudy:
        raise ValueError(
            "Number of LD matrices ({}) does not match number of studies ({})".format(
                len(R_list), nstudy
            )
        )
    
    # Validate dimensions
    n_variants = len(variant_ids)
    for i, R in enumerate(R_list):
        if R.shape[0] != n_variants or R.shape[1] != n_variants:
            raise ValueError(
                "LD matrix {} has shape {} but expected ({}, {})".format(
                    i+1, R.shape, n_variants, n_variants
                )
            )
    
    # Convert scalar rho to correlation matrix if needed
    # MultiSuSiE expects either a scalar or a KxK correlation matrix
    # If scalar, create a matrix with diagonal=1 and off-diagonal=rho
    log.write(" -Processing rho parameter...", verbose=kwargs.get("verbose", True))
    rho_original = rho
    if np.isscalar(rho):
        K = nstudy
        rho_matrix = np.full((K, K), float(rho), dtype=float_type)
        np.fill_diagonal(rho_matrix, 1.0)
        rho = rho_matrix
        log.write("  -Converted scalar rho={} to {}x{} correlation matrix".format(
            rho_original, K, K
        ), verbose=kwargs.get("verbose", True))
        log.write("  -Rho matrix:\n{}".format(rho), verbose=kwargs.get("verbose", True))
    elif isinstance(rho, (list, np.ndarray)):
        # Ensure it's a numpy array
        rho = np.asarray(rho, dtype=float_type)
        log.write("  -Rho is already a matrix with shape {}".format(rho.shape), 
                 verbose=kwargs.get("verbose", True))
    
    # Run MultiSuSiE
    log.write(" -Running MultiSuSiE RSS...", verbose=kwargs.get("verbose", True))
    
    # Log final inputs being passed to MultiSuSiE
    log.write("  -Final inputs summary:", verbose=kwargs.get("verbose", True))
    log.write("    -Number of variants: {}".format(len(variant_ids)), verbose=kwargs.get("verbose", True))
    log.write("    -Population sizes: {}".format(population_sizes), verbose=kwargs.get("verbose", True))
    log.write("    -varY_list: {}".format([f'{v:.6f}' for v in varY_list]), verbose=kwargs.get("verbose", True))
    log.write("    -R_list shapes: {}".format([R.shape for R in R_list]), verbose=kwargs.get("verbose", True))
    log.write("    -b_list shapes: {}".format([b.shape for b in b_list]), verbose=kwargs.get("verbose", True))
    log.write("    -s_list shapes: {}".format([s.shape for s in s_list]), verbose=kwargs.get("verbose", True))
    log.write("    -rho shape: {}".format(rho.shape if hasattr(rho, 'shape') else 'scalar'), 
             verbose=kwargs.get("verbose", True))
    
    # Log causal variants in final inputs
    if len(variant_ids) > 38:
        causal_indices = [3, 10, 38]
        log.write("  -Causal variants in final inputs (indices 3, 10, 38):", 
                 verbose=kwargs.get("verbose", True))
        for idx in causal_indices:
            log.write("    Index {} ({}):".format(idx, variant_ids[idx] if idx < len(variant_ids) else 'N/A'), 
                     verbose=kwargs.get("verbose", True))
            for pop_idx in range(nstudy):
                log.write("      Pop {}: BETA={:.6f}, SE={:.6f}".format(
                    pop_idx+1, b_list[pop_idx][idx], s_list[pop_idx][idx]
                ), verbose=kwargs.get("verbose", True))
    
    # Temporarily suppress numpy division warnings/errors
    # MultiSuSiE handles these internally
    old_settings = np.seterr(all='ignore')
    try:
        log.write("  -Calling multisusie_rss()...", verbose=kwargs.get("verbose", True))
        result = multisusie_rss(
        R_list=R_list,
        population_sizes=population_sizes,
        b_list=b_list,
        s_list=s_list,
        z_list=z_list,
        varY_list=varY_list,
        rho=rho,
        L=L,
        scaled_prior_variance=scaled_prior_variance,
        prior_weights=prior_weights,
        standardize=standardize,
        pop_spec_standardization=pop_spec_standardization,
        estimate_residual_variance=estimate_residual_variance,
        estimate_prior_variance=estimate_prior_variance,
        estimate_prior_method=estimate_prior_method,
        pop_spec_effect_priors=pop_spec_effect_priors,
        iter_before_zeroing_effects=iter_before_zeroing_effects,
        prior_tol=prior_tol,
        max_iter=max_iter,
        tol=tol,
        verbose=verbose,
        coverage=coverage,
        min_abs_corr=min_abs_corr,
        float_type=float_type,
        low_memory_mode=low_memory_mode,
        recover_R=recover_R,
        single_population_mac_thresh=single_population_mac_thresh,
        mac_list=mac_list,
        multi_population_maf_thresh=multi_population_maf_thresh,
        maf_list=maf_list,
        variant_ids=variant_ids,
        **kwargs
        )
    finally:
        # Restore numpy error settings
        np.seterr(**old_settings)
    
    # Log MultiSuSiE results
    log.write("  -MultiSuSiE RSS completed", verbose=kwargs.get("verbose", True))
    log.write("    -Converged: {}".format(result.converged if hasattr(result, 'converged') else 'N/A'), 
             verbose=kwargs.get("verbose", True))
    log.write("    -Number of iterations: {}".format(result.niter if hasattr(result, 'niter') else 'N/A'), 
             verbose=kwargs.get("verbose", True))
    log.write("    -PIP shape: {}".format(result.pip.shape if hasattr(result, 'pip') else 'N/A'), 
             verbose=kwargs.get("verbose", True))
    
    # Log causal variant PIPs from MultiSuSiE result
    if len(variant_ids) > 38:
        causal_indices = [3, 10, 38]
        log.write("  -Causal variants PIPs from MultiSuSiE (indices 3, 10, 38):", 
                 verbose=kwargs.get("verbose", True))
        for idx in causal_indices:
            pip_val = result.pip[idx] if hasattr(result, 'pip') and idx < len(result.pip) else 'N/A'
            log.write("    Index {} ({}): PIP={:.6f}".format(
                idx, variant_ids[idx] if idx < len(variant_ids) else 'N/A', pip_val
            ), verbose=kwargs.get("verbose", True))
    
    # Convert results to DataFrame
    log.write(" -Converting results to DataFrame...", verbose=kwargs.get("verbose", True))
    
    # Create results DataFrame
    # Ensure variant_ids and result.pip are aligned
    results_df = pd.DataFrame({
        "SNPID": variant_ids,
        "PIP": result.pip
    })
    
    # Add CHR and POS if available - align by SNPID to ensure correct mapping
    if "CHR" in data.columns and "POS" in data.columns:
        # Create a mapping from SNPID to CHR/POS
        chr_pos_map = data.set_index("SNPID")[["CHR", "POS"]].to_dict('index')
        results_df["CHR"] = results_df["SNPID"].map(lambda x: chr_pos_map.get(x, {}).get("CHR", None))
        results_df["POS"] = results_df["SNPID"].map(lambda x: chr_pos_map.get(x, {}).get("POS", None))
    elif "CHR" in data.columns:
        chr_map = data.set_index("SNPID")["CHR"].to_dict()
        results_df["CHR"] = results_df["SNPID"].map(chr_map)
    elif "POS" in data.columns:
        pos_map = data.set_index("SNPID")["POS"].to_dict()
        results_df["POS"] = results_df["SNPID"].map(pos_map)
    
    # Add credible set information
    if hasattr(result, 'sets') and result.sets is not None:
        # Initialize credible set columns
        results_df["CREDIBLE_SET_INDEX"] = 0
        results_df["CS_CATEGORY"] = None
        
        if len(result.sets) >= 1 and len(result.sets[0]) > 0:
            # result.sets[0] contains indices of variants in each credible set
            for cs_idx, variant_indices in enumerate(result.sets[0]):
                if variant_indices is not None and len(variant_indices) > 0:
                    results_df.loc[variant_indices, "CREDIBLE_SET_INDEX"] = cs_idx + 1
                    
                    # Add purity if available
                    if len(result.sets) > 1 and result.sets[1] is not None:
                        if cs_idx < len(result.sets[1]):
                            purity = result.sets[1][cs_idx]
                            results_df.loc[variant_indices, "CS_PURITY"] = purity
                    
                    # Add coverage if available
                    if len(result.sets) > 2 and result.sets[2] is not None:
                        if cs_idx < len(result.sets[2]):
                            coverage_val = result.sets[2][cs_idx]
                            results_df.loc[variant_indices, "CS_COVERAGE"] = coverage_val
    
    # Add effect size estimates if available
    if hasattr(result, 'coef') and result.coef is not None:
        for i in range(nstudy):
            results_df["BETA_EST_{}".format(i+1)] = result.coef[i, :]
    
    if hasattr(result, 'coef_sd') and result.coef_sd is not None:
        for i in range(nstudy):
            results_df["BETA_SD_{}".format(i+1)] = result.coef_sd[i, :]
    
    # Add convergence information
    if hasattr(result, 'converged'):
        results_df.attrs['converged'] = result.converged
    if hasattr(result, 'niter'):
        results_df.attrs['niter'] = result.niter
    
    log.write(" -MultiSuSiE RSS completed. Found {} credible sets.".format(
        len(results_df[results_df["CREDIBLE_SET_INDEX"] > 0].groupby("CREDIBLE_SET_INDEX"))
    ), verbose=kwargs.get("verbose", True))
    
    # Log final results for causal variants
    if len(variant_ids) > 38:
        causal_indices = [3, 10, 38]
        log.write("  -Final results for causal variants:", verbose=kwargs.get("verbose", True))
        for idx in causal_indices:
            if idx < len(results_df):
                row = results_df.iloc[idx]
                log.write("    Index {} ({}): PIP={:.6f}, CS_INDEX={}".format(
                    idx, row['SNPID'], row['PIP'], row['CREDIBLE_SET_INDEX']
                ), verbose=kwargs.get("verbose", True))
    
    return results_df
